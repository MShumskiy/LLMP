{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "class ModelOperatorOllama():\n",
    "    def __init__(self):\n",
    "        self.url = 'http://127.0.0.1:11435/api/'\n",
    "        self.model_list = self.list_models()\n",
    "        \n",
    "        \n",
    "\n",
    "    def generate_response(self,model,system_prompt,prompt,format=None):\n",
    "        \"\"\"\n",
    "        Sends a request to the LLM API and returns the response.\n",
    "        \"\"\"\n",
    "        \n",
    "        if model in self.model_list:\n",
    "            self.model = model\n",
    "        else:\n",
    "            raise ValueError(f\"Model '{model}' not provided or not found in available models: {self.model_list}\")\n",
    "        \n",
    "        url=\"http://127.0.0.1:11435/api/chat\"\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"keep_alive\": 0,\n",
    "            \"messages\": [\n",
    "                {'role':'system',\n",
    "                'content':system_prompt\n",
    "                },\n",
    "                {'role':'user',\n",
    "                'content':prompt}\n",
    "                ],\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        if format:\n",
    "            payload.update(format)\n",
    "\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        \n",
    "        response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # Return the JSON response\n",
    "        else:\n",
    "            return {\"error\": f\"Request failed with status {response.status_code}\", \"details\": response.text}\n",
    "        \n",
    "    # def unload_model(self):\n",
    "    #     \"\"\"\n",
    "    #     Sends a request to the LLM API to set keep_alive parameter.\n",
    "    #     \"\"\"\n",
    "    #     url=\"http://127.0.0.1:11435/api/chat\"\n",
    "    #     payload = {\n",
    "    #         \"model\": self.model,\n",
    "    #         \"keep_alive\": 0,\n",
    "    #         \"stream\": False\n",
    "    #     }\n",
    "\n",
    "    #     headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    #     response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "    #     if response.status_code == 200:\n",
    "    #         return response.json()  # Return the JSON response\n",
    "    #     else:\n",
    "    #         return {\"error\": f\"Request failed with status {response.status_code}\", \"details\": response.text}\n",
    "\n",
    "    def list_models(self):\n",
    "        \n",
    "        response = requests.get(f'{self.url}tags')\n",
    "        model_dict = {\n",
    "            model['name']: {'model_name': model['name'],\n",
    "                            'param_size': model['details']['parameter_size'],\n",
    "                            'quant_level': model['details']['quantization_level']} for model in response.json()['models']\n",
    "    }\n",
    "        return model_dict\n",
    "\n",
    "# response = generate_response('you are a comedian',f'say 2', model=\"llama3.2\")\n",
    "# llmp = ModelOperatorOllama('phi4:latest')\n",
    "# system_prompt = 'you are a finance expert who can understand any piece of news and its implications on the markets. You excel at forex forecasting. Given the following piece of news, make your forecast'\n",
    "\n",
    "# prompt = df_news.iloc[0]['article_text']\n",
    "\n",
    "# format = {'format':{\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"currency\": {\n",
    "#             \"type\": \"string\",\n",
    "#             \"enum\": [\"USD\", \"EUR\", \"GBP\", \"JPY\", \"INR\"],\n",
    "#             \"description\": \"Currency code most affected by this news.\"\n",
    "#         },\n",
    "#         \"amount\": {\n",
    "#             \"type\": \"integer\",\n",
    "#             \"description\": \"Sign of the signal, 0 for negative and 1 for positive.\"\n",
    "#         },\n",
    "#         \"strenght\": {\n",
    "#             \"type\": \"number\",\n",
    "#             \"description\": \"strenght of the signal, 0 for minumum and 1 for maximum.\"\n",
    "#         },\n",
    "#         \"type\": {\n",
    "#             \"type\": \"string\",\n",
    "#             \"enum\": [\"descriptive\",\"predictive\"],\n",
    "#             \"description\": \"whether the signal refers to past or future\"\n",
    "#         }\n",
    "#     },\n",
    "#     \"required\": [\"currency\", \"amount\"],\n",
    "# }}\n",
    "\n",
    "# llmp.generate_response('','write numbers to 10')\n",
    "# response = llmp.generate_response(system_prompt,prompt,format)\n",
    "# llmp.list_models()\n",
    "# llmp.unload_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmp = ModelOperatorOllama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llmp.generate_response('llama3.2:latest','','what is my name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any information about you, so I'm not aware of your name. This conversation just started, and I'm here to help answer any questions or provide information on a wide range of topics. Would you like to tell me your name?\n"
     ]
    }
   ],
   "source": [
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qwen2.5:14b': {'model_name': 'qwen2.5:14b',\n",
       "  'param_size': '14.8B',\n",
       "  'quant_level': 'Q4_K_M'},\n",
       " 'phi4:latest': {'model_name': 'phi4:latest',\n",
       "  'param_size': '14.7B',\n",
       "  'quant_level': 'Q4_K_M'},\n",
       " 'deepseek-r1:14b': {'model_name': 'deepseek-r1:14b',\n",
       "  'param_size': '14.8B',\n",
       "  'quant_level': 'Q4_K_M'},\n",
       " 'llama3.2:latest': {'model_name': 'llama3.2:latest',\n",
       "  'param_size': '3.2B',\n",
       "  'quant_level': 'Q4_K_M'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmp.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelOperatorOllama' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 54\u001b[0m, in \u001b[0;36mModelOperatorOllama.unload_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mSends a request to the LLM API to set keep_alive parameter.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:11435/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m,\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     57\u001b[0m }\n\u001b[0;32m     59\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     61\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(payload), headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModelOperatorOllama' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "llmp.unload_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
